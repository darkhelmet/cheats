
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../torchvision/">
      
      
        <link rel="next" href="../../tools/protobuf/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.5.17">
    
    
      
        <title>Transformers (Hugging Face) - Cheat Sheets</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.bcfcd587.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#transformers-hugging-face" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Cheat Sheets" class="md-header__button md-logo" aria-label="Cheat Sheets" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Cheat Sheets
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Transformers (Hugging Face)
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Cheat Sheets" class="md-nav__button md-logo" aria-label="Cheat Sheets" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    Cheat Sheets
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Cheat Sheets Collection
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../machine-learning-algorithms/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Machine Learning Algorithms
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Gpu
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Gpu
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../gpu/cuda/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    CUDA Programming
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Javascript
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Javascript
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../javascript/nextjs/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Next.js
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../javascript/react/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    React
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Os
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Os
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../os/bottlerocket/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Bottlerocket OS Administration
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" checked>
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Python
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Python
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../inquirer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Python Inquirer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../keras/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Keras
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../langchain/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LangChain
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../langextract/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LangExtract
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../matplotlib/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Matplotlib
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nltk/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    NLTK (Natural Language Toolkit)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../numpy/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    NumPy
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pandas/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Pandas
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pillow/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Pillow (PIL)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../polars/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Polars
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../python/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Python
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pytorch/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../scikit-learn/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Scikit-learn
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../scipy/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    SciPy
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../seaborn/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Seaborn
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../sentence-transformers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Sentence-Transformers (UKPLab)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tensorflow/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TensorFlow
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../torchvision/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TorchVision
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Transformers (Hugging Face)
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Transformers (Hugging Face)
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#installation" class="md-nav__link">
    <span class="md-ellipsis">
      Installation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#basic-setup" class="md-nav__link">
    <span class="md-ellipsis">
      Basic Setup
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#core-functionality" class="md-nav__link">
    <span class="md-ellipsis">
      Core Functionality
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Core Functionality">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pipeline-api-quickstart" class="md-nav__link">
    <span class="md-ellipsis">
      Pipeline API (Quickstart)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-and-tokenizer-loading" class="md-nav__link">
    <span class="md-ellipsis">
      Model and Tokenizer Loading
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tokenization" class="md-nav__link">
    <span class="md-ellipsis">
      Tokenization
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#common-use-cases" class="md-nav__link">
    <span class="md-ellipsis">
      Common Use Cases
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Common Use Cases">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#text-classification" class="md-nav__link">
    <span class="md-ellipsis">
      Text Classification
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#text-generation" class="md-nav__link">
    <span class="md-ellipsis">
      Text Generation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-answering" class="md-nav__link">
    <span class="md-ellipsis">
      Question Answering
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#text-summarization" class="md-nav__link">
    <span class="md-ellipsis">
      Text Summarization
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#advanced-features" class="md-nav__link">
    <span class="md-ellipsis">
      Advanced Features
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Advanced Features">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#fine-tuning-models" class="md-nav__link">
    <span class="md-ellipsis">
      Fine-tuning Models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#custom-model-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      Custom Model Architecture
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#working-with-different-modalities" class="md-nav__link">
    <span class="md-ellipsis">
      Working with Different Modalities
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#integration-with-other-libraries" class="md-nav__link">
    <span class="md-ellipsis">
      Integration with Other Libraries
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Integration with Other Libraries">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#with-datasets-library" class="md-nav__link">
    <span class="md-ellipsis">
      With Datasets Library
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#with-pytorch-lightning" class="md-nav__link">
    <span class="md-ellipsis">
      With PyTorch Lightning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#with-gradio-for-web-apps" class="md-nav__link">
    <span class="md-ellipsis">
      With Gradio for Web Apps
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#best-practices" class="md-nav__link">
    <span class="md-ellipsis">
      Best Practices
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Best Practices">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#memory-and-performance-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      Memory and Performance Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-evaluation-and-monitoring" class="md-nav__link">
    <span class="md-ellipsis">
      Model Evaluation and Monitoring
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#error-handling-and-robust-inference" class="md-nav__link">
    <span class="md-ellipsis">
      Error Handling and Robust Inference
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#real-world-examples" class="md-nav__link">
    <span class="md-ellipsis">
      Real-world Examples
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Real-world Examples">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#complete-text-classification-system" class="md-nav__link">
    <span class="md-ellipsis">
      Complete Text Classification System
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multi-task-nlp-pipeline" class="md-nav__link">
    <span class="md-ellipsis">
      Multi-task NLP Pipeline
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Tools
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Tools
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tools/protobuf/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Protocol Buffers (protobuf)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tools/ripgrep/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Ripgrep (rg)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tools/vim-lazyvim/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Vim/Neovim with LazyVim
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#installation" class="md-nav__link">
    <span class="md-ellipsis">
      Installation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#basic-setup" class="md-nav__link">
    <span class="md-ellipsis">
      Basic Setup
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#core-functionality" class="md-nav__link">
    <span class="md-ellipsis">
      Core Functionality
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Core Functionality">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pipeline-api-quickstart" class="md-nav__link">
    <span class="md-ellipsis">
      Pipeline API (Quickstart)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-and-tokenizer-loading" class="md-nav__link">
    <span class="md-ellipsis">
      Model and Tokenizer Loading
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tokenization" class="md-nav__link">
    <span class="md-ellipsis">
      Tokenization
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#common-use-cases" class="md-nav__link">
    <span class="md-ellipsis">
      Common Use Cases
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Common Use Cases">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#text-classification" class="md-nav__link">
    <span class="md-ellipsis">
      Text Classification
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#text-generation" class="md-nav__link">
    <span class="md-ellipsis">
      Text Generation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-answering" class="md-nav__link">
    <span class="md-ellipsis">
      Question Answering
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#text-summarization" class="md-nav__link">
    <span class="md-ellipsis">
      Text Summarization
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#advanced-features" class="md-nav__link">
    <span class="md-ellipsis">
      Advanced Features
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Advanced Features">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#fine-tuning-models" class="md-nav__link">
    <span class="md-ellipsis">
      Fine-tuning Models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#custom-model-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      Custom Model Architecture
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#working-with-different-modalities" class="md-nav__link">
    <span class="md-ellipsis">
      Working with Different Modalities
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#integration-with-other-libraries" class="md-nav__link">
    <span class="md-ellipsis">
      Integration with Other Libraries
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Integration with Other Libraries">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#with-datasets-library" class="md-nav__link">
    <span class="md-ellipsis">
      With Datasets Library
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#with-pytorch-lightning" class="md-nav__link">
    <span class="md-ellipsis">
      With PyTorch Lightning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#with-gradio-for-web-apps" class="md-nav__link">
    <span class="md-ellipsis">
      With Gradio for Web Apps
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#best-practices" class="md-nav__link">
    <span class="md-ellipsis">
      Best Practices
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Best Practices">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#memory-and-performance-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      Memory and Performance Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-evaluation-and-monitoring" class="md-nav__link">
    <span class="md-ellipsis">
      Model Evaluation and Monitoring
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#error-handling-and-robust-inference" class="md-nav__link">
    <span class="md-ellipsis">
      Error Handling and Robust Inference
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#real-world-examples" class="md-nav__link">
    <span class="md-ellipsis">
      Real-world Examples
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Real-world Examples">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#complete-text-classification-system" class="md-nav__link">
    <span class="md-ellipsis">
      Complete Text Classification System
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multi-task-nlp-pipeline" class="md-nav__link">
    <span class="md-ellipsis">
      Multi-task NLP Pipeline
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="transformers-hugging-face">Transformers (Hugging Face)</h1>
<p>Transformers is Hugging Face's flagship library providing state-of-the-art machine learning models for PyTorch, TensorFlow, and JAX. It offers thousands of pretrained models to perform tasks on different domains like text, vision, and audio.</p>
<h2 id="installation">Installation</h2>
<pre><code class="language-bash"># Basic installation
pip install transformers

# With PyTorch
pip install transformers[torch]

# With TensorFlow
pip install transformers[tf]

# With additional dependencies
pip install transformers[torch,vision,audio]

# Development version
pip install git+https://github.com/huggingface/transformers.git

# With specific backend
pip install transformers torch torchvision torchaudio
</code></pre>
<h2 id="basic-setup">Basic Setup</h2>
<pre><code class="language-python">from transformers import (
    AutoTokenizer, AutoModel, AutoModelForSequenceClassification,
    AutoModelForCausalLM, AutoModelForMaskedLM,
    pipeline, Trainer, TrainingArguments,
    BertTokenizer, BertModel, GPT2LMHeadModel,
    T5ForConditionalGeneration, T5Tokenizer
)

import torch
import numpy as np
from datasets import load_dataset
</code></pre>
<h2 id="core-functionality">Core Functionality</h2>
<h3 id="pipeline-api-quickstart">Pipeline API (Quickstart)</h3>
<pre><code class="language-python"># Sentiment analysis
classifier = pipeline(&quot;sentiment-analysis&quot;)
result = classifier(&quot;I love this product!&quot;)
print(result)  # [{'label': 'POSITIVE', 'score': 0.9998}]

# Text generation
generator = pipeline(&quot;text-generation&quot;, model=&quot;gpt2&quot;)
result = generator(&quot;The future of AI is&quot;, max_length=50, num_return_sequences=2)
print(result[0]['generated_text'])

# Question answering
qa_pipeline = pipeline(&quot;question-answering&quot;)
context = &quot;The capital of France is Paris. Paris is known for the Eiffel Tower.&quot;
question = &quot;What is the capital of France?&quot;
answer = qa_pipeline(question=question, context=context)
print(answer)  # {'answer': 'Paris', 'score': 0.9999, 'start': 23, 'end': 28}

# Named Entity Recognition
ner = pipeline(&quot;ner&quot;, aggregation_strategy=&quot;simple&quot;)
text = &quot;My name is Wolfgang and I live in Berlin&quot;
entities = ner(text)
print(entities)

# Summarization
summarizer = pipeline(&quot;summarization&quot;, model=&quot;facebook/bart-large-cnn&quot;)
article = &quot;Your long article text here...&quot;
summary = summarizer(article, max_length=130, min_length=30, do_sample=False)
print(summary[0]['summary_text'])

# Translation
translator = pipeline(&quot;translation_en_to_fr&quot;)
result = translator(&quot;Hello, how are you?&quot;)
print(result[0]['translation_text'])  # Bonjour, comment allez-vous?

# Fill mask
unmasker = pipeline(&quot;fill-mask&quot;)
result = unmasker(&quot;Paris is the capital of &lt;mask&gt;.&quot;)
print(result[0]['token_str'])  # France
</code></pre>
<h3 id="model-and-tokenizer-loading">Model and Tokenizer Loading</h3>
<pre><code class="language-python"># Auto classes (recommended)
model_name = &quot;bert-base-uncased&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

# Specific model classes
tokenizer = BertTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
model = BertModel.from_pretrained(&quot;bert-base-uncased&quot;)

# Loading with specific configurations
from transformers import BertConfig
config = BertConfig.from_pretrained(&quot;bert-base-uncased&quot;)
config.num_hidden_layers = 6  # Modify configuration
model = BertModel.from_pretrained(&quot;bert-base-uncased&quot;, config=config)

# Loading from local files
tokenizer = AutoTokenizer.from_pretrained(&quot;./my_model_directory&quot;)
model = AutoModel.from_pretrained(&quot;./my_model_directory&quot;)

# Loading with specific torch dtype
model = AutoModel.from_pretrained(
    model_name, 
    torch_dtype=torch.float16,
    device_map=&quot;auto&quot;
)
</code></pre>
<h3 id="tokenization">Tokenization</h3>
<pre><code class="language-python"># Basic tokenization
text = &quot;Hello, how are you today?&quot;
tokens = tokenizer.tokenize(text)
print(tokens)  # ['hello', ',', 'how', 'are', 'you', 'today', '?']

# Encoding (text to tokens to IDs)
encoding = tokenizer(text)
print(encoding['input_ids'])  # [101, 7592, 1010, 2129, 2024, 2017, 2651, 1029, 102]
print(encoding['attention_mask'])  # [1, 1, 1, 1, 1, 1, 1, 1, 1]

# Batch encoding
texts = [&quot;Hello world!&quot;, &quot;How are you?&quot;, &quot;Fine, thanks!&quot;]
batch_encoding = tokenizer(
    texts,
    padding=True,
    truncation=True,
    max_length=128,
    return_tensors=&quot;pt&quot;  # PyTorch tensors
)

# Decoding (IDs back to text)
decoded = tokenizer.decode(encoding['input_ids'])
print(decoded)  # [CLS] hello, how are you today? [SEP]

# Advanced tokenization options
encoding = tokenizer(
    text,
    add_special_tokens=True,    # Add [CLS] and [SEP]
    max_length=512,            # Maximum sequence length
    padding=&quot;max_length&quot;,      # Pad to max_length
    truncation=True,           # Truncate if longer
    return_attention_mask=True, # Return attention masks
    return_token_type_ids=True, # Return token type IDs (for BERT)
    return_tensors=&quot;pt&quot;        # Return PyTorch tensors
)
</code></pre>
<h2 id="common-use-cases">Common Use Cases</h2>
<h3 id="text-classification">Text Classification</h3>
<pre><code class="language-python">from transformers import AutoModelForSequenceClassification
import torch.nn.functional as F

# Load model for classification
model_name = &quot;cardiffnlp/twitter-roberta-base-sentiment-latest&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

def classify_sentiment(text):
    # Tokenize
    inputs = tokenizer(text, return_tensors=&quot;pt&quot;, truncation=True, max_length=512)

    # Get predictions
    with torch.no_grad():
        outputs = model(**inputs)
        predictions = F.softmax(outputs.logits, dim=-1)

    # Get predicted class
    predicted_class = torch.argmax(predictions, dim=-1).item()
    confidence = predictions[0][predicted_class].item()

    # Map to labels (model specific)
    labels = [&quot;negative&quot;, &quot;neutral&quot;, &quot;positive&quot;]
    return {
        &quot;label&quot;: labels[predicted_class],
        &quot;confidence&quot;: confidence
    }

# Usage
result = classify_sentiment(&quot;I love this new feature!&quot;)
print(result)  # {'label': 'positive', 'confidence': 0.9234}
</code></pre>
<h3 id="text-generation">Text Generation</h3>
<pre><code class="language-python">from transformers import GPT2LMHeadModel, GPT2Tokenizer

# Load GPT-2 model
tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)
model = GPT2LMHeadModel.from_pretrained(&quot;gpt2&quot;)

# Add pad token
tokenizer.pad_token = tokenizer.eos_token

def generate_text(prompt, max_length=100, temperature=0.7, num_return_sequences=1):
    # Tokenize input
    inputs = tokenizer.encode(prompt, return_tensors=&quot;pt&quot;)

    # Generate
    with torch.no_grad():
        outputs = model.generate(
            inputs,
            max_length=max_length,
            temperature=temperature,
            num_return_sequences=num_return_sequences,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
            repetition_penalty=1.2
        )

    # Decode results
    generated_texts = []
    for output in outputs:
        text = tokenizer.decode(output, skip_special_tokens=True)
        generated_texts.append(text)

    return generated_texts

# Usage
texts = generate_text(&quot;The future of artificial intelligence&quot;, max_length=80)
for text in texts:
    print(text)
</code></pre>
<h3 id="question-answering">Question Answering</h3>
<pre><code class="language-python">from transformers import AutoModelForQuestionAnswering

# Load QA model
model_name = &quot;distilbert-base-cased-distilled-squad&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForQuestionAnswering.from_pretrained(model_name)

def answer_question(context, question):
    # Tokenize
    inputs = tokenizer.encode_plus(
        question, context,
        add_special_tokens=True,
        return_tensors=&quot;pt&quot;,
        max_length=512,
        truncation=True
    )

    # Get predictions
    with torch.no_grad():
        outputs = model(**inputs)
        start_scores = outputs.start_logits
        end_scores = outputs.end_logits

    # Find the best answer
    start_idx = torch.argmax(start_scores)
    end_idx = torch.argmax(end_scores) + 1

    # Decode answer
    input_ids = inputs['input_ids'][0]
    answer_tokens = input_ids[start_idx:end_idx]
    answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)

    # Calculate confidence
    confidence = (start_scores[0][start_idx] + end_scores[0][end_idx-1]).item()

    return {
        &quot;answer&quot;: answer,
        &quot;confidence&quot;: confidence,
        &quot;start&quot;: start_idx.item(),
        &quot;end&quot;: end_idx.item()
    }

# Usage
context = &quot;&quot;&quot;
The Transformer architecture was introduced in the paper &quot;Attention Is All You Need&quot; 
by Vaswani et al. in 2017. It revolutionized natural language processing by using 
self-attention mechanisms instead of recurrent layers.
&quot;&quot;&quot;

question = &quot;When was the Transformer architecture introduced?&quot;
result = answer_question(context, question)
print(result)  # {'answer': '2017', 'confidence': 15.23, ...}
</code></pre>
<h3 id="text-summarization">Text Summarization</h3>
<pre><code class="language-python">from transformers import T5ForConditionalGeneration, T5Tokenizer

# Load T5 model for summarization
model_name = &quot;t5-small&quot;
tokenizer = T5Tokenizer.from_pretrained(model_name)
model = T5ForConditionalGeneration.from_pretrained(model_name)

def summarize_text(text, max_length=150, min_length=50):
    # T5 requires task prefix
    input_text = &quot;summarize: &quot; + text

    # Tokenize
    inputs = tokenizer.encode(
        input_text,
        return_tensors=&quot;pt&quot;,
        max_length=512,
        truncation=True
    )

    # Generate summary
    with torch.no_grad():
        summary_ids = model.generate(
            inputs,
            max_length=max_length,
            min_length=min_length,
            length_penalty=2.0,
            num_beams=4,
            early_stopping=True
        )

    # Decode summary
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return summary

# Usage
article = &quot;&quot;&quot;
Artificial Intelligence (AI) has become increasingly important in modern technology. 
Machine learning algorithms can now process vast amounts of data and make predictions 
with remarkable accuracy. Deep learning, a subset of machine learning, uses neural 
networks with multiple layers to learn complex patterns in data. This technology 
powers many applications we use daily, from search engines to recommendation systems.
&quot;&quot;&quot;

summary = summarize_text(article)
print(summary)
</code></pre>
<h2 id="advanced-features">Advanced Features</h2>
<h3 id="fine-tuning-models">Fine-tuning Models</h3>
<pre><code class="language-python">from transformers import Trainer, TrainingArguments, DataCollatorWithPadding
from datasets import Dataset
import torch

# Prepare dataset
def prepare_dataset():
    # Your data preparation logic
    texts = [&quot;positive example&quot;, &quot;negative example&quot;, ...]
    labels = [1, 0, ...]  # Binary classification

    return Dataset.from_dict({
        &quot;text&quot;: texts,
        &quot;labels&quot;: labels
    })

# Tokenize dataset
def tokenize_function(examples):
    return tokenizer(
        examples[&quot;text&quot;],
        truncation=True,
        padding=True,
        max_length=128
    )

# Load model for training
model = AutoModelForSequenceClassification.from_pretrained(
    &quot;bert-base-uncased&quot;,
    num_labels=2
)

# Prepare data
train_dataset = prepare_dataset()
train_dataset = train_dataset.map(tokenize_function, batched=True)

# Set training arguments
training_args = TrainingArguments(
    output_dir=&quot;./results&quot;,
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=64,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir=&quot;./logs&quot;,
    logging_steps=10,
    evaluation_strategy=&quot;epoch&quot;,
    save_strategy=&quot;epoch&quot;,
    load_best_model_at_end=True,
    metric_for_best_model=&quot;accuracy&quot;,
)

# Data collator
data_collator = DataCollatorWithPadding(tokenizer)

# Define compute metrics function
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)

    accuracy = accuracy_score(labels, predictions)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')

    return {
        'accuracy': accuracy,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

# Create trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=train_dataset,  # Use validation set in practice
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

# Train the model
trainer.train()

# Save the model
trainer.save_model(&quot;./fine_tuned_model&quot;)
</code></pre>
<h3 id="custom-model-architecture">Custom Model Architecture</h3>
<pre><code class="language-python">import torch.nn as nn
from transformers import BertPreTrainedModel, BertModel

class CustomBertClassifier(BertPreTrainedModel):
    def __init__(self, config, num_labels=2):
        super().__init__(config)
        self.num_labels = num_labels

        self.bert = BertModel(config)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

        # Custom classifier head
        self.classifier = nn.Sequential(
            nn.Linear(config.hidden_size, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, num_labels)
        )

        self.init_weights()

    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None):
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids
        )

        pooled_output = outputs[1]  # [CLS] token representation
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)

        loss = None
        if labels is not None:
            loss_fct = nn.CrossEntropyLoss()
            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))

        return {
            'loss': loss,
            'logits': logits,
            'hidden_states': outputs.hidden_states,
            'attentions': outputs.attentions
        }

# Usage
config = AutoConfig.from_pretrained(&quot;bert-base-uncased&quot;)
model = CustomBertClassifier(config, num_labels=3)
</code></pre>
<h3 id="working-with-different-modalities">Working with Different Modalities</h3>
<pre><code class="language-python"># Vision-Language Models (CLIP)
from transformers import CLIPProcessor, CLIPModel
from PIL import Image
import requests

model = CLIPModel.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;)
processor = CLIPProcessor.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;)

# Load image
url = &quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;
image = Image.open(requests.get(url, stream=True).raw)

# Process inputs
inputs = processor(
    text=[&quot;a photo of a cat&quot;, &quot;a photo of a dog&quot;],
    images=image,
    return_tensors=&quot;pt&quot;,
    padding=True
)

# Get similarities
outputs = model(**inputs)
logits_per_image = outputs.logits_per_image
probs = logits_per_image.softmax(dim=1)
print(probs)  # Probability for each text description

# Audio Models (Wav2Vec2)
from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC
import librosa

# Load audio model
processor = Wav2Vec2Processor.from_pretrained(&quot;facebook/wav2vec2-base-960h&quot;)
model = Wav2Vec2ForCTC.from_pretrained(&quot;facebook/wav2vec2-base-960h&quot;)

# Process audio file
audio, sampling_rate = librosa.load(&quot;path_to_audio.wav&quot;, sr=16000)
inputs = processor(audio, sampling_rate=16000, return_tensors=&quot;pt&quot;, padding=True)

# Get transcription
with torch.no_grad():
    logits = model(inputs.input_values).logits

predicted_ids = torch.argmax(logits, dim=-1)
transcription = processor.batch_decode(predicted_ids)[0]
print(transcription)
</code></pre>
<h2 id="integration-with-other-libraries">Integration with Other Libraries</h2>
<h3 id="with-datasets-library">With Datasets Library</h3>
<pre><code class="language-python">from datasets import load_dataset, Dataset, DatasetDict

# Load popular datasets
dataset = load_dataset(&quot;imdb&quot;)  # Movie reviews
dataset = load_dataset(&quot;squad&quot;)  # Question answering
dataset = load_dataset(&quot;glue&quot;, &quot;sst2&quot;)  # Sentiment analysis

# Create custom dataset
data = {
    &quot;text&quot;: [&quot;Great movie!&quot;, &quot;Terrible film.&quot;, &quot;Average story.&quot;],
    &quot;label&quot;: [2, 0, 1]  # positive, negative, neutral
}
custom_dataset = Dataset.from_dict(data)

# Tokenize dataset
def tokenize_data(examples):
    return tokenizer(examples[&quot;text&quot;], truncation=True, padding=&quot;max_length&quot;, max_length=128)

tokenized_dataset = custom_dataset.map(tokenize_data, batched=True)

# Split dataset
train_test = custom_dataset.train_test_split(test_size=0.2)
dataset_dict = DatasetDict({
    'train': train_test['train'],
    'test': train_test['test']
})
</code></pre>
<h3 id="with-pytorch-lightning">With PyTorch Lightning</h3>
<pre><code class="language-python">import pytorch_lightning as pl
from torch.utils.data import DataLoader

class TransformersLightningModule(pl.LightningModule):
    def __init__(self, model_name=&quot;bert-base-uncased&quot;, num_labels=2, learning_rate=2e-5):
        super().__init__()
        self.save_hyperparameters()

        self.model = AutoModelForSequenceClassification.from_pretrained(
            model_name, num_labels=num_labels
        )
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)

    def forward(self, input_ids, attention_mask, labels=None):
        return self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)

    def training_step(self, batch, batch_idx):
        outputs = self(**batch)
        loss = outputs.loss
        self.log(&quot;train_loss&quot;, loss)
        return loss

    def validation_step(self, batch, batch_idx):
        outputs = self(**batch)
        loss = outputs.loss
        self.log(&quot;val_loss&quot;, loss)
        return loss

    def configure_optimizers(self):
        return torch.optim.AdamW(self.parameters(), lr=self.hparams.learning_rate)

# Usage
model = TransformersLightningModule()
trainer = pl.Trainer(max_epochs=3, gpus=1 if torch.cuda.is_available() else 0)
trainer.fit(model, train_dataloader, val_dataloader)
</code></pre>
<h3 id="with-gradio-for-web-apps">With Gradio for Web Apps</h3>
<pre><code class="language-python">import gradio as gr
from transformers import pipeline

# Create sentiment analysis pipeline
sentiment_pipeline = pipeline(&quot;sentiment-analysis&quot;, 
                             model=&quot;cardiffnlp/twitter-roberta-base-sentiment-latest&quot;)

def analyze_sentiment(text):
    result = sentiment_pipeline(text)[0]
    return f&quot;Sentiment: {result['label']} (Confidence: {result['score']:.3f})&quot;

# Create Gradio interface
interface = gr.Interface(
    fn=analyze_sentiment,
    inputs=gr.Textbox(lines=3, placeholder=&quot;Enter text to analyze...&quot;),
    outputs=&quot;text&quot;,
    title=&quot;Sentiment Analysis&quot;,
    description=&quot;Analyze the sentiment of your text using RoBERTa model&quot;
)

# Launch the app
interface.launch()
</code></pre>
<h2 id="best-practices">Best Practices</h2>
<h3 id="memory-and-performance-optimization">Memory and Performance Optimization</h3>
<pre><code class="language-python"># 1. Use appropriate model sizes
model = AutoModel.from_pretrained(
    &quot;distilbert-base-uncased&quot;,  # Smaller, faster alternative to BERT
    torch_dtype=torch.float16,  # Half precision for memory efficiency
    device_map=&quot;auto&quot;           # Automatic device placement
)

# 2. Batch processing for efficiency
def batch_inference(texts, batch_size=32):
    results = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        batch_encoding = tokenizer(
            batch,
            truncation=True,
            padding=True,
            return_tensors=&quot;pt&quot;
        )

        with torch.no_grad():
            outputs = model(**batch_encoding)
            results.extend(outputs.logits.cpu().numpy())

    return results

# 3. Gradient checkpointing for training large models
model.gradient_checkpointing_enable()

# 4. Use DataLoader with multiple workers
from torch.utils.data import DataLoader
dataloader = DataLoader(
    dataset,
    batch_size=16,
    num_workers=4,
    pin_memory=True
)
</code></pre>
<h3 id="model-evaluation-and-monitoring">Model Evaluation and Monitoring</h3>
<pre><code class="language-python">from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

def evaluate_model(model, tokenizer, test_texts, test_labels):
    predictions = []

    for text in test_texts:
        inputs = tokenizer(text, return_tensors=&quot;pt&quot;, truncation=True, max_length=128)

        with torch.no_grad():
            outputs = model(**inputs)
            pred = torch.argmax(outputs.logits, dim=-1).item()
            predictions.append(pred)

    # Classification report
    report = classification_report(test_labels, predictions, output_dict=True)
    print(classification_report(test_labels, predictions))

    # Confusion matrix
    cm = confusion_matrix(test_labels, predictions)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title('Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.show()

    return report, cm

# Usage
report, cm = evaluate_model(model, tokenizer, test_texts, test_labels)
</code></pre>
<h3 id="error-handling-and-robust-inference">Error Handling and Robust Inference</h3>
<pre><code class="language-python">def robust_inference(text, model, tokenizer, max_retries=3):
    &quot;&quot;&quot;Robust inference with error handling&quot;&quot;&quot;
    for attempt in range(max_retries):
        try:
            # Validate input
            if not isinstance(text, str) or not text.strip():
                return {&quot;error&quot;: &quot;Invalid input text&quot;}

            # Tokenize with length check
            inputs = tokenizer(
                text,
                return_tensors=&quot;pt&quot;,
                truncation=True,
                max_length=512,
                padding=True
            )

            # Check if input is too long even after truncation
            if inputs['input_ids'].shape[1] &gt; 512:
                return {&quot;error&quot;: &quot;Text too long even after truncation&quot;}

            # Inference
            with torch.no_grad():
                outputs = model(**inputs)
                predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
                predicted_class = torch.argmax(predictions, dim=-1).item()
                confidence = predictions[0][predicted_class].item()

            return {
                &quot;prediction&quot;: predicted_class,
                &quot;confidence&quot;: confidence,
                &quot;success&quot;: True
            }

        except Exception as e:
            if attempt == max_retries - 1:
                return {&quot;error&quot;: f&quot;Inference failed after {max_retries} attempts: {str(e)}&quot;}
            time.sleep(0.1)  # Brief pause before retry

    return {&quot;error&quot;: &quot;Maximum retries exceeded&quot;}
</code></pre>
<h2 id="real-world-examples">Real-world Examples</h2>
<h3 id="complete-text-classification-system">Complete Text Classification System</h3>
<pre><code class="language-python">class TextClassificationSystem:
    def __init__(self, model_name=&quot;distilbert-base-uncased&quot;, num_labels=3):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(
            model_name, num_labels=num_labels
        )
        self.label_map = {0: &quot;negative&quot;, 1: &quot;neutral&quot;, 2: &quot;positive&quot;}

    def preprocess_text(self, text):
        &quot;&quot;&quot;Clean and preprocess text&quot;&quot;&quot;
        import re
        text = re.sub(r'http\S+', '', text)  # Remove URLs
        text = re.sub(r'@\w+', '', text)     # Remove mentions
        text = re.sub(r'#\w+', '', text)     # Remove hashtags
        text = re.sub(r'\s+', ' ', text)     # Normalize whitespace
        return text.strip()

    def predict(self, text):
        &quot;&quot;&quot;Make prediction on single text&quot;&quot;&quot;
        cleaned_text = self.preprocess_text(text)

        inputs = self.tokenizer(
            cleaned_text,
            return_tensors=&quot;pt&quot;,
            truncation=True,
            max_length=128,
            padding=True
        )

        with torch.no_grad():
            outputs = self.model(**inputs)
            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
            predicted_class = torch.argmax(predictions, dim=-1).item()
            confidence = predictions[0][predicted_class].item()

        return {
            &quot;text&quot;: text,
            &quot;cleaned_text&quot;: cleaned_text,
            &quot;prediction&quot;: self.label_map[predicted_class],
            &quot;confidence&quot;: confidence,
            &quot;all_scores&quot;: {
                self.label_map[i]: predictions[0][i].item() 
                for i in range(len(self.label_map))
            }
        }

    def predict_batch(self, texts, batch_size=32):
        &quot;&quot;&quot;Make predictions on batch of texts&quot;&quot;&quot;
        results = []

        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            cleaned_texts = [self.preprocess_text(text) for text in batch_texts]

            inputs = self.tokenizer(
                cleaned_texts,
                return_tensors=&quot;pt&quot;,
                truncation=True,
                max_length=128,
                padding=True
            )

            with torch.no_grad():
                outputs = self.model(**inputs)
                predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
                predicted_classes = torch.argmax(predictions, dim=-1)

                for j, (text, cleaned_text) in enumerate(zip(batch_texts, cleaned_texts)):
                    pred_class = predicted_classes[j].item()
                    confidence = predictions[j][pred_class].item()

                    results.append({
                        &quot;text&quot;: text,
                        &quot;cleaned_text&quot;: cleaned_text,
                        &quot;prediction&quot;: self.label_map[pred_class],
                        &quot;confidence&quot;: confidence
                    })

        return results

# Usage
classifier = TextClassificationSystem()

# Single prediction
result = classifier.predict(&quot;I absolutely love this new product! It's amazing!&quot;)
print(result)

# Batch prediction
texts = [
    &quot;Great service and friendly staff!&quot;,
    &quot;Terrible experience, would not recommend.&quot;,
    &quot;It was okay, nothing special.&quot;
]
results = classifier.predict_batch(texts)
for result in results:
    print(f&quot;{result['prediction']}: {result['text']} (confidence: {result['confidence']:.3f})&quot;)
</code></pre>
<h3 id="multi-task-nlp-pipeline">Multi-task NLP Pipeline</h3>
<pre><code class="language-python">class MultiTaskNLPPipeline:
    def __init__(self):
        # Initialize different models for different tasks
        self.sentiment_analyzer = pipeline(&quot;sentiment-analysis&quot;)
        self.ner_model = pipeline(&quot;ner&quot;, aggregation_strategy=&quot;simple&quot;)
        self.qa_model = pipeline(&quot;question-answering&quot;)
        self.summarizer = pipeline(&quot;summarization&quot;, model=&quot;facebook/bart-large-cnn&quot;)

    def analyze_text(self, text, tasks=[&quot;sentiment&quot;, &quot;ner&quot;, &quot;summary&quot;]):
        &quot;&quot;&quot;Comprehensive text analysis&quot;&quot;&quot;
        results = {&quot;original_text&quot;: text}

        if &quot;sentiment&quot; in tasks:
            sentiment = self.sentiment_analyzer(text)[0]
            results[&quot;sentiment&quot;] = {
                &quot;label&quot;: sentiment[&quot;label&quot;],
                &quot;confidence&quot;: sentiment[&quot;score&quot;]
            }

        if &quot;ner&quot; in tasks:
            entities = self.ner_model(text)
            results[&quot;entities&quot;] = [
                {
                    &quot;text&quot;: entity[&quot;word&quot;],
                    &quot;label&quot;: entity[&quot;entity_group&quot;],
                    &quot;confidence&quot;: entity[&quot;score&quot;]
                }
                for entity in entities
            ]

        if &quot;summary&quot; in tasks and len(text.split()) &gt; 30:
            try:
                summary = self.summarizer(text, max_length=100, min_length=30)[0]
                results[&quot;summary&quot;] = summary[&quot;summary_text&quot;]
            except:
                results[&quot;summary&quot;] = &quot;Text too short for summarization&quot;

        return results

    def answer_questions(self, context, questions):
        &quot;&quot;&quot;Answer multiple questions about a context&quot;&quot;&quot;
        answers = []
        for question in questions:
            try:
                answer = self.qa_model(question=question, context=context)
                answers.append({
                    &quot;question&quot;: question,
                    &quot;answer&quot;: answer[&quot;answer&quot;],
                    &quot;confidence&quot;: answer[&quot;score&quot;]
                })
            except:
                answers.append({
                    &quot;question&quot;: question,
                    &quot;answer&quot;: &quot;Could not answer&quot;,
                    &quot;confidence&quot;: 0.0
                })
        return answers

# Usage
nlp_pipeline = MultiTaskNLPPipeline()

text = &quot;&quot;&quot;
Apple Inc. is an American multinational technology company headquartered in Cupertino, California. 
Apple is the world's largest technology company by revenue and the world's most valuable company. 
The company was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne in April 1976. 
Apple's products include the iPhone, iPad, Mac, Apple Watch, and Apple TV.
&quot;&quot;&quot;

# Comprehensive analysis
analysis = nlp_pipeline.analyze_text(text)
print(&quot;Analysis Results:&quot;)
print(f&quot;Sentiment: {analysis['sentiment']['label']} ({analysis['sentiment']['confidence']:.3f})&quot;)
print(f&quot;Entities: {[e['text'] for e in analysis['entities']]}&quot;)
print(f&quot;Summary: {analysis['summary']}&quot;)

# Question answering
questions = [
    &quot;Who founded Apple?&quot;,
    &quot;Where is Apple headquartered?&quot;,
    &quot;What products does Apple make?&quot;
]
answers = nlp_pipeline.answer_questions(text, questions)
for qa in answers:
    print(f&quot;Q: {qa['question']}&quot;)
    print(f&quot;A: {qa['answer']} (confidence: {qa['confidence']:.3f})&quot;)
</code></pre>
<p>This comprehensive cheat sheet covers the essential aspects of the Transformers library. The library's strength lies in its unified API across different models and tasks, extensive model zoo, and seamless integration with the broader ML ecosystem. It's the go-to library for state-of-the-art NLP applications and research.</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": [], "search": "../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.1e8ae164.min.js"></script>
      
        <script src="../../search/main.js"></script>
      
    
  </body>
</html>